import streamlit as st
import base64
import sounddevice as sd
from scipy.io.wavfile import write
import openai
import tempfile
import os
import time
from io import BytesIO
import numpy as np
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

client = openai.OpenAI(api_key=api_key)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë©€í‹°ëª¨ë‹¬ ì–´ì‹œìŠ¤í„´íŠ¸", page_icon="ğŸ¤–", layout="wide")

# íƒ€ì´í‹€ ë° ì†Œê°œ
st.title("ğŸ¤– ë©€í‹°ëª¨ë‹¬ ì–´ì‹œìŠ¤í„´íŠ¸")
st.write("ì´ë¯¸ì§€ì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  ìŒì„±ìœ¼ë¡œ ë‹µë³€ì„ ë“¤ì–´ë³´ì„¸ìš”!")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "conversations" not in st.session_state:
    st.session_state.conversations = []
if "current_image" not in st.session_state:
    st.session_state.current_image = None
if "chat_active" not in st.session_state:
    st.session_state.chat_active = False

# ì‚¬ì´ë“œë°”: OpenAI API í‚¤ ì…ë ¥
with st.sidebar:
    st.header("ì„¤ì •")

    # ë…¹ìŒ ì„¤ì •
    st.subheader("ë…¹ìŒ ì„¤ì •")
    seconds = st.slider("ë…¹ìŒ ì‹œê°„ (ì´ˆ)", min_value=1, max_value=10, value=5)
    fs = st.selectbox("ìƒ˜í”Œë§ ë ˆì´íŠ¸", [16000, 22050, 44100], index=0)

# ë©”ì¸ í™”ë©´ ë¶„í• 
col1, col2 = st.columns([1, 1])

# ì´ë¯¸ì§€ ì—…ë¡œë“œ ì„¹ì…˜
with col1:
    st.header("ì´ë¯¸ì§€ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader(
        "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "jpeg", "png"]
    )

    if uploaded_file is not None:
        st.image(uploaded_file, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_container_width=True)
        st.session_state.current_image = uploaded_file.getvalue()
        st.session_state.chat_active = True

        if st.button("ìƒˆ ëŒ€í™” ì‹œì‘"):
            st.session_state.conversations = []
            st.rerun()

# ì‹¤ì‹œê°„ ë…¹ìŒ ì„¹ì…˜
with col2:
    st.header("ì§ˆë¬¸í•˜ê¸°")
    text_question = st.text_input("í…ìŠ¤íŠ¸ë¡œ ì§ˆë¬¸:", key="text_question")
    st.subheader("ë˜ëŠ” ìŒì„±ìœ¼ë¡œ ì§ˆë¬¸:")

    if st.button("ğŸ™ï¸ ë…¹ìŒ ì‹œì‘"):
        if st.session_state.current_image is None:
            st.error("ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
        else:
            with st.spinner(f"ğŸ™ï¸ {seconds}ì´ˆ ë™ì•ˆ ë…¹ìŒ ì¤‘..."):
                temp_audio_file = tempfile.NamedTemporaryFile(
                    delete=False, suffix=".wav"
                )
                temp_audio_filename = temp_audio_file.name
                temp_audio_file.close()

                progress = st.progress(0)
                recording = sd.rec(
                    int(seconds * fs), samplerate=fs, channels=1, dtype="int16"
                )
                for i in range(seconds):
                    progress.progress((i + 1) / seconds)
                    time.sleep(1)
                sd.wait()
                write(temp_audio_filename, fs, recording)
                st.success("âœ… ë…¹ìŒ ì™„ë£Œ!")

                try:
                    with open(temp_audio_filename, "rb") as audio_file:
                        transcript = client.audio.transcriptions.create(
                            file=audio_file,
                            model="whisper-1",
                            language="ko",
                            response_format="text",
                        )
                    st.session_state.current_question = transcript.strip()
                    st.write(f"ğŸ™‹ ì¸ì‹ëœ ì§ˆë¬¸: {st.session_state.current_question}")

                except Exception as e:
                    st.error(f"ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                finally:
                    os.unlink(temp_audio_filename)

    if st.button("ì¢…ë£Œ", key="quit_button"):
        st.session_state.chat_active = False
        st.write("ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ë ¤ë©´ í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")
        st.stop()


# ì§ˆë¬¸ ì²˜ë¦¬ í•¨ìˆ˜
def process_question(question):
    if st.session_state.current_image is None:
        st.error("ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return

    base64_image = base64.b64encode(st.session_state.current_image).decode("utf-8")

    with st.spinner("ğŸ” GPTê°€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„ì¤‘..."):
        try:
            completion = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": question},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                },
                            },
                        ],
                    }
                ],
            )

            english_description = completion.choices[0].message.content

            translation = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "Translate the following English text into natural Korean.",
                    },
                    {"role": "user", "content": english_description},
                ],
            )

            korean_translation = translation.choices[0].message.content.strip()

            response = client.audio.speech.create(
                model="tts-1", voice="nova", input=korean_translation
            )

            st.session_state.conversations.append(
                {
                    "question": question,
                    "answer": korean_translation,
                    "audio": response.content,
                }
            )

            return korean_translation, response.content

        except Exception as e:
            st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return None, None


current_question = None
if text_question:
    current_question = text_question
elif "current_question" in st.session_state:
    current_question = st.session_state.current_question
    st.session_state.current_question = None

if current_question and st.session_state.chat_active:
    answer, audio_content = process_question(current_question)
    if answer and audio_content:
        st.session_state.text_question = ""

if st.session_state.conversations:
    st.header("ëŒ€í™” ê¸°ë¡")
    for i, conv in enumerate(st.session_state.conversations):
        with st.expander(
            f"ì§ˆë¬¸ {i+1}: {conv['question']}",
            expanded=(i == len(st.session_state.conversations) - 1),
        ):
            st.write(conv["answer"])
            st.audio(conv["audio"], format="audio/mp3")
