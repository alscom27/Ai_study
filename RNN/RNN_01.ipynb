{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666bf020",
   "metadata": {},
   "source": [
    "### 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c9c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65184115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
      "       [  101,  1045,  5223,  2023,  2061,  2172,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]])>, 'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c058e40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
      "[1045, 5223, 2023, 2061, 2172]\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 2061, 2172, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "tokens = [tokenizer.tokenize(sentence) for sentence in raw_inputs]\n",
    "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "\n",
    "print(ids[0])\n",
    "print(ids[1])\n",
    "print(tokenizer(raw_inputs, padding=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "359a14c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Let's try to tokenize!\")\n",
    "\n",
    "print(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3086d79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] let ' s try to tokenize! [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcabaa8",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36383942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.2426779e-03  9.2993546e-03 -1.9766092e-04 -1.9672764e-03\n",
      "  4.6036304e-03 -4.0953159e-03  2.7431143e-03  6.9399667e-03\n",
      "  6.0654259e-03 -7.5107943e-03  9.3823504e-03  4.6718083e-03\n",
      "  3.9661205e-03 -6.2435055e-03  8.4599797e-03 -2.1501649e-03\n",
      "  8.8251876e-03 -5.3620026e-03 -8.1294188e-03  6.8245591e-03\n",
      "  1.6711927e-03 -2.1985089e-03  9.5136007e-03  9.4938548e-03\n",
      " -9.7740470e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
      "  2.0227872e-03  4.3050171e-04  6.7363144e-04 -3.8206363e-03\n",
      " -7.1402504e-03 -2.0888723e-03  3.9238976e-03  8.8186832e-03\n",
      "  9.2591504e-03 -5.9759365e-03 -9.4026709e-03  9.7643770e-03\n",
      "  3.4297847e-03  5.1661171e-03  6.2823449e-03 -2.8042626e-03\n",
      "  7.3227035e-03  2.8302716e-03  2.8710044e-03 -2.3803699e-03\n",
      " -3.1282497e-03 -2.3701417e-03  4.2764368e-03  7.6057913e-05\n",
      " -9.5842788e-03 -9.6655441e-03 -6.1481940e-03 -1.2856961e-04\n",
      "  1.9974159e-03  9.4319675e-03  5.5843508e-03 -4.2906962e-03\n",
      "  2.7831673e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
      "  4.3234206e-03 -5.8143795e-03 -8.0419064e-04  8.1000505e-03\n",
      " -2.3600650e-03 -9.6634552e-03  5.7792603e-03 -3.9298222e-03\n",
      " -1.2228728e-03  9.9805174e-03 -2.2563506e-03 -4.7570644e-03\n",
      " -5.3293873e-03  6.9808899e-03 -5.7088719e-03  2.1136629e-03\n",
      " -5.2556600e-03  6.1207139e-03  4.3573068e-03  2.6063549e-03\n",
      " -1.4910829e-03 -2.7460635e-03  8.9929365e-03  5.2157748e-03\n",
      " -2.1625196e-03 -9.4703101e-03 -7.4260519e-03 -1.0637414e-03\n",
      " -7.9494715e-04 -2.5629092e-03  9.6827205e-03 -4.5852066e-04\n",
      "  5.8737611e-03 -7.4475873e-03 -2.5060738e-03 -5.5498634e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 텍스트 데이터를 전처리하여 단어의 시퀀스로 변환\n",
    "sentences = [['I', 'love', 'NLP'], ['Word2Vec', 'is', 'awesome']]\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# 단어의 벡터 표현 확인\n",
    "vector = model.wv['NLP']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88546521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\main\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a5b4766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'love', 'natural', 'language', 'processing'], ['Word2Vec', 'is', 'a', 'popular', 'wold', 'embedding', 'model'], ['Natural', 'language', 'processing', 'is', 'an', 'important', 'field', 'in', 'AI']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 문장 데이터\n",
    "sentences = [\n",
    "    'I love natural language processing',\n",
    "    'Word2Vec is a popular wold embedding model',\n",
    "    'Natural language processing is an important field in AI'\n",
    "]\n",
    "\n",
    "# 문장을 토큰화\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75db3069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.3780823e-04  2.3722864e-04  5.1040910e-03  9.0102609e-03\n",
      " -9.3020601e-03 -7.1179681e-03  6.4588394e-03  8.9740995e-03\n",
      " -5.0159963e-03 -3.7642447e-03  7.3804674e-03 -1.5347273e-03\n",
      " -4.5378106e-03  6.5551274e-03 -4.8596673e-03 -1.8148653e-03\n",
      "  2.8774342e-03  9.9292281e-04 -8.2860254e-03 -9.4493292e-03\n",
      "  7.3125120e-03  5.0700661e-03  6.7592775e-03  7.6191645e-04\n",
      "  6.3517732e-03 -3.4046052e-03 -9.4711396e-04  5.7690116e-03\n",
      " -7.5220009e-03 -3.9353538e-03 -7.5102169e-03 -9.3084743e-04\n",
      "  9.5383907e-03 -7.3201652e-03 -2.3331069e-03 -1.9370844e-03\n",
      "  8.0786459e-03 -5.9302277e-03  4.5893696e-05 -4.7526849e-03\n",
      " -9.6026389e-03  5.0062062e-03 -8.7609617e-03 -4.3915613e-03\n",
      " -3.5224024e-05 -2.9514002e-04 -7.6609026e-03  9.6143829e-03\n",
      "  4.9826237e-03  9.2333835e-03 -8.1571704e-03  4.4943946e-03\n",
      " -4.1372897e-03  8.2511746e-04  8.4984917e-03 -4.4609406e-03\n",
      "  4.5189071e-03 -6.7863516e-03 -3.5486098e-03  9.3998490e-03\n",
      " -1.5785332e-03  3.2206855e-04 -4.1410322e-03 -7.6832231e-03\n",
      " -1.5072766e-03  2.4704486e-03 -8.8708906e-04  5.5336859e-03\n",
      " -2.7418635e-03  2.2610263e-03  5.4554208e-03  8.3450489e-03\n",
      " -1.4531317e-03 -9.2072617e-03  4.3708128e-03  5.7099562e-04\n",
      "  7.4411072e-03 -8.1354554e-04 -2.6376632e-03 -8.7539414e-03\n",
      " -8.5795758e-04  2.8268313e-03  5.4021459e-03  7.0519913e-03\n",
      " -5.7032341e-03  1.8587466e-03  6.0890075e-03 -4.7994759e-03\n",
      " -3.1069717e-03  6.7969002e-03  1.6318457e-03  1.9004464e-04\n",
      "  3.4740919e-03  2.1673259e-04  9.6185161e-03  5.0612199e-03\n",
      " -8.9166937e-03 -7.0420718e-03  8.9997036e-04  6.3935458e-03]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec 모델 학습\n",
    "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# 단어의 벡터 표현 확인\n",
    "word_vectors = model.wv\n",
    "print(word_vectors['language']) # 'language' 단어의 벡터 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
