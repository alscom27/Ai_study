{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd44fb46",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7240684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 수입\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from time import time\n",
    "import numpy as np\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00b3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 지정\n",
    "src_vocab = 2500 # 입력 문장에 사용하는 단어 종류\n",
    "tgt_vocab = 3000 # 출력 문장에 사용하는 단어 종류\n",
    "d_model = 512 # 단어 임베딩 차원수\n",
    "num_heads = 8 # multi-head attention 계산할 머리 수\n",
    "num_layers = 6 # 인코더/디코더 적층 수\n",
    "\n",
    "d_ff = 2048 # feed through 층의 뉴런 갯수\n",
    "max_seq = 100 # 입력 문장의 최대 길이\n",
    "tot_epoch = 4 # 반복 학습 수\n",
    "batch_size = 5 # 배치 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0691bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 모양 torch.Size([5, 100])\n",
      "tensor([2012, 1285, 1342,  559, 1166, 1495, 2398, 2420,  964,  713,   94,   88,\n",
      "        1815, 2471, 1657, 2102, 1635, 1801, 1483, 2324, 1367, 2461, 1227,  684,\n",
      "        2366,  777, 1187,   39,  370, 1642, 2139, 2250, 1429, 1577,  100,  773,\n",
      "        1597,  102, 2499,   69, 1866, 2137, 2163,   95,  455, 2012,  959, 2060,\n",
      "        1290, 1616, 1065,  887, 1457, 1734, 2046, 2228,  543, 1724, 1349,  470,\n",
      "         700, 2435, 1714, 1646, 2386, 2198, 1453, 1572, 2492, 1733, 2460,  449,\n",
      "        2191, 1339, 1511, 1299, 1203, 2111,  986, 2149,  385,  704, 1631, 1847,\n",
      "        1494, 2472, 1533, 1466, 1586, 1727,  895, 2446, 2008,  953, 2405, 2296,\n",
      "        2102, 1887, 1491,  618])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 임의 생성\n",
    "\n",
    "# 입력 데이터\n",
    "src_data = torch.randint(0, src_vocab, (batch_size, max_seq))\n",
    "print(f'입력 데이터 모양 {src_data.shape}')\n",
    "print(src_data[0])\n",
    "\n",
    "# 출력 데이터\n",
    "tgt_data = torch.randint(0, tgt_vocab, (batch_size, max_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea1fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention class 구현\n",
    "\n",
    "class My_MHA(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(My_MHA, self).__init__()\n",
    "        self.d_model = d_model # 512 차원\n",
    "        self.num_heads = num_heads # 헤드 8개\n",
    "        \n",
    "        # 각 머리 당 처리할 차원 수\n",
    "        self.d_head = d_model // num_heads # 512/8 = 64(헤드당 64개 차원 처리)\n",
    "        # print(f'각 머리 당 처리할 차원 수 {self.d_head}')\n",
    "        \n",
    "        # Q, K, V 행렬 준비(W_Q, W_K, W_V에 학습 가능 파라미터 262, 656개 모델 생성)\n",
    "        self.W_Q = nn.Linear(d_model, d_model) # 512 입력 512 출력(512*512+512=262,656)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 머리 나누기\n",
    "        # 입력 데이터 모양 : [5, 100, 512]\n",
    "        # 출력 데이터 모양 : [5, 8, 100, 64]\n",
    "        def split_heads(self, x):\n",
    "            batch_size = x.size(0)\n",
    "            # PyTorch 텐서의 첫 번째 차원을 읽어서 현재 입력 배치의\n",
    "            # 크기를 추출, 입력 x의 실제 배치 크기에 맞춰 자동 적용\n",
    "            # (x:[B, T, d_model] = [5, 100, 512])\n",
    "            # x = x.view(batch_size, -1, self.num_heads, self.d_head)\n",
    "            # transpose(dim1, dim2)\n",
    "            # 텐서에서 **두 개의 차원(axis)**을 서로 교환\n",
    "            # num_heads를 앞으로 재위치하여 head별 연산을 준비\n",
    "            # 데이터 모양 : [5, 100, 8, 64]\n",
    "            x = x.view(batch_size, -1, self.num_heads, self.d_head)\n",
    "            x = x.transpose(1,2) # 헤드별로 처리하기 위한 교환\n",
    "            return x\n",
    "        \n",
    "        # 유사성 게산\n",
    "        def dot_prod(self, Q, K, V, mask):\n",
    "            score = torch.matmul(Q, K.transpose(-1, 2)) / math.sqrt(self.d_head)\n",
    "            # 어텐션 스코어(attention score)**를 계산\n",
    "            # K.transpose(-1, -2): Key의 전치\n",
    "            # K: Shape [batch, heads, seq_len, d_head]\n",
    "            # K.transpose(-1, -2) → [batch, heads, d_head, seq_len]\n",
    "            # Q와 K의 내적이 가능하도록 마지막 두 차원 바꿈\n",
    "            # torch.matmul(Q, Kᵀ)결과 Shape: [batch, heads, seq_len, seq_len]\n",
    "            # 각 위치 간의 연관성 (attention score)을 계산\n",
    "            # math.sqrt(self.d_head)스케일링 (정규화):\n",
    "            # d_head가 클수록 값이 커져 softmax의 gradient가 작아지는\n",
    "            # 문제를 완화하기 위함, 따라서 √64 = 8로 나누어 스케일 조정\n",
    "            \n",
    "            # 디코더 마스크 처리\n",
    "            if mask is not None:\n",
    "                score = score.masked_fill(mask == 0, -1e9)\n",
    "            prob = torch.softmax(score, dim=-1)\n",
    "            \n",
    "            # V 행렬과 곱셈\n",
    "            Z = torch.matmul(prob, V)\n",
    "            return Z\n",
    "        \n",
    "        # 머리 합치기\n",
    "        # 입력 데이터 모양 : [5, 8, 100, 64]\n",
    "        # 출력 데이터 모양 : [5, 100, 512]\n",
    "        def combine_heads(self, x):\n",
    "            batch_size = x.size(0)\n",
    "            # 데이터 모양 : [5, 100, 8, 64]\n",
    "            Z = x.transpose(1,2)\n",
    "            \n",
    "            # 데이터 모양 : [5, 100, 512], 입력과 동일한 출력형태\n",
    "            Z = Z.contiguous().view(batch_size, -1, self.d_model)\n",
    "            return Z\n",
    "        \n",
    "        # 앞서 정의한 함수로 forward를 정의\n",
    "        def forward(self, q, k, v, mask=None):\n",
    "            # 머리 나누기 실행/생성된 모델에 임베딩된 토큰(문자열 q,k,v)입력\n",
    "            Q = self.split_heads(self.W_Q(q))\n",
    "            K = self.split_heads(self.W_K(k))\n",
    "            V = self.split_heads(self.W_V(v))\n",
    "            \n",
    "            # 유사성 계산 실행\n",
    "            attn = self.dot_prod(Q, K, V, mask)\n",
    "            \n",
    "            # 머리 합치기 실행\n",
    "            Z = self.combine_heads(attn)\n",
    "            \n",
    "            return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3fe69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward class 구현\n",
    "\n",
    "class My_FFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(My_FFN, self).__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4fe5337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어 트랜스포머 인코더 구현\n",
    "\n",
    "class My_Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(My_Encoder, self).__init__()\n",
    "        \n",
    "        self.mha = My_MHA(d_model, num_heads) # 멀티헤드 셀프 어텐션 -> 입력 토큰끼리 상호 작용\n",
    "        self.ffn = My_FFN(d_model, d_ff) # 포지션별 피드포워드 신경망 -> 비선형 변환\n",
    "        self.layer_norm = nn.LayerNorm(d_model) # 레이어 정규화 -> 학습 안정성, 기울기 흐름 개선\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        z = self.mha(x,x,x,mask)\n",
    "        # self.mha(x, x, x)?\n",
    "        # \"Query = Key = Value = 동일한 입력 시퀀스 x\"라는 뜻\n",
    "        # 트랜스포머 구조에서 통상적으로 사용하는 표현 방식으로\n",
    "        # 형태는 같으나 궁극적으로 서로 달라지는 q,k v\n",
    "        z = self.layer_norm(x + z) # 잔차 연결\n",
    "        w = self.ffn(z)\n",
    "        z = self.layer_norm(w + z) # 레이어 정규화, 잔차 연결\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c57a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위치 인코딩 구현\n",
    "\n",
    "class My_Position(nn.Module):\n",
    "    def __init__(self, d_model, max_seq):\n",
    "        super(My_Position, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model # 512차원\n",
    "        self.max_seq = max_seq # 입력 문장 최대 길이 100\n",
    "        \n",
    "    # 위치 임베딩 계산\n",
    "    def pos_enc(self, x):\n",
    "        k = torch.arange(0, self.max_seq, 1).float() # 0부터 100까지의 실수\n",
    "        print(f'전 {k.shape}') # [100]\n",
    "        k = k.unsqueeze(1)\n",
    "        print(f'후 {k.shape}') # [100, 1] 2차원 텐서로 변형\n",
    "        result = torch.zeros(self.max_seq, self.d_model)\n",
    "        print(f'결과 {result.shape}') # [100, 512] 텐서 공간에 0으로 세팅, 포지셔널 벡터용 공간\n",
    "        twoi = torch.arange(0, self.d_model, 2).float() # 512 차원의 짝수 인덱스 생성\n",
    "        print(twoi)\n",
    "        \n",
    "        result[:, 0::2] = torch.sin(k / (10000**(twoi / self.d_model))) # k는 입력 토큰\n",
    "        result[:, 1::2] = torch.cos(k / (10000**(twoi / self.d_model)))\n",
    "        result = result.unsqueeze(0)\n",
    "        print(f'최종 모양 {result.shape}')\n",
    "        return result\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pos = self.pos_enc(x)\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "166d5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 구현\n",
    "\n",
    "class My_Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(My_Decoder, self).__init__()\n",
    "        \n",
    "        self.mha_1 = My_MHA(d_model, num_heads)\n",
    "        self.mha_2 = My_MHA(d_model, num_heads)\n",
    "        self.ffn = My_FFN(d_model, d_ff)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
    "        print(f'디코더 입력 데이터 모양 {x.shape}')\n",
    "        \n",
    "        # 디코더 self attention 부분\n",
    "        z = self.mha_1(x,x,x, tgt_mask)\n",
    "        z = self.layer_norm(x + z)\n",
    "        print(f'self attention 후 데이터 모양 {x.shape}')\n",
    "        \n",
    "        y = self.mha_2(z, enc_out, enc_out, src_mask)\n",
    "        y = self.layer_norm(z + y)\n",
    "        \n",
    "        # feed forward 부분\n",
    "        w = self.ffn(y)\n",
    "        z = self.layer_norm(w + y)\n",
    "        print(f'feed forward 후 데이터 모양 {z.shape}')\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3540d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 자연어 트랜스포머 구성\n",
    "\n",
    "class My_Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, d_ff, max_seq):\n",
    "        super(My_Transformer, self).__init__()\n",
    "        \n",
    "        self.enc_embed = nn.Embedding(src_vocab, d_model) # 2500개가 512차원으로 임베딩\n",
    "        self.dec_embed = nn.Embedding(tgt_vocab, d_model) # 3000개가 512차원으로 임베딩\n",
    "        self.pos_enc = My_Position(d_model, max_seq)\n",
    "        \n",
    "        # 인코더 쌓기\n",
    "        self.enc_layers = nn.ModuleList(\n",
    "            [My_Encoder(d_model, num_heads, d_ff)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        # 디코더 쌓기\n",
    "        self.dec_layers = nn.ModuleList(\n",
    "            [My_Decoder(d_model, num_heads, d_ff)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        # 최종 출력층\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # 디코더 마스크 제작\n",
    "        def make_mask(self, src, tgt):\n",
    "            src_mask = None\n",
    "            tgt_mask = tgt.unsqueeze(1).unsqueeze(3)\n",
    "            tmp = torch.ones(1, max_seq, max_seq)\n",
    "            mask = torch.tril(tmp).bool()\n",
    "            print(f'틀 {mask}')\n",
    "            \n",
    "            tgt_mask = tgt_mask * mask\n",
    "            print(f'마스크 모양', tgt_mask.shape)\n",
    "            print(f'마스크 결과', tgt_mask)\n",
    "            return src_mask, tgt_mask\n",
    "        \n",
    "        def forward(self, src, tgt):\n",
    "            # 마스크 만들기\n",
    "            src_mask, tgt_mask = self.make_mask(src, tgt)\n",
    "            print(f'마스크 모양 {tgt_mask.shape}')\n",
    "            print(tgt_mask[1])\n",
    "            \n",
    "            # 단어 임베딩 및 위치 정보 추가\n",
    "            src_embed = self.enc_embed(src)\n",
    "            tgt_embed = self.dec_embed(tgt)\n",
    "            src_pos = self.pos_enc(src)\n",
    "            tgt_pos = self.pos_enc(tgt)\n",
    "            src_embed = src_embed + src_pos\n",
    "            tgt_embed = tgt_embed + tgt_pos\n",
    "            \n",
    "            # 인코더 연결\n",
    "            enc_out = src_embed\n",
    "            for layer in self.enc_layer:\n",
    "                enc_out = layer(enc_out, src_mask)\n",
    "            \n",
    "            # 디코더 연결\n",
    "            dec_out = tgt_embed\n",
    "            for layer in self.dec_layers:\n",
    "                dec_out = layer(dec_out, enc_out, src_mask, tgt_mask)\n",
    "            \n",
    "            # 최종 출력\n",
    "            out = self.linear(dec_out)\n",
    "            out = self.softmax(out)\n",
    "            out = torch.argmax(out, dim=-1)\n",
    "            print(f'최종 출력 모양 {out.shape}')\n",
    "            return out\n",
    "        \n",
    "        # 테스트 코드\n",
    "        temp = My_Transformer(d_model, num_heads, num_layers, d_ff, max_seq)\n",
    "        z = temp(src_data, tgt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392c464a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 가중치 수 출력하기\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Moudles\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f'전체 모델 가중치 수 : {total_params}')\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머 학습\n",
    "\n",
    "model = My_Transformer(d_model, num_heads, num_layers, d_ff, max_seq)\n",
    "# count_parameter(model)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'전체 모델 가중치수 : {total_params}')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 학습 시작\n",
    "begin = time()\n",
    "print('학습 시작')\n",
    "for epoch in range(tot_epoch):\n",
    "    print('epoch', epoch, '시작')\n",
    "    for batch in range(batch_size):\n",
    "        output = model(src_data, tgt_data)\n",
    "        pred = output.view(-1, tgt_vocab)\n",
    "        truth = tgt_data.view(-1).long()\n",
    "        \n",
    "        loss = criterion(pred, truth)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(' batch', batch, 'done.')\n",
    "print('학습 종료')\n",
    "end=time()\n",
    "print('걸린 시간', end-begin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
