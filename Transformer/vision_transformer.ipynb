{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e6cbd4",
   "metadata": {},
   "source": [
    "### Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19dac172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 수입\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8338d21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 장치: cuda\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼 파라미터 지정\n",
    "\n",
    "MY_SHAPE = (1, 28, 28) # 입력 이미지 형태 (MNIST: 흑백 28x28)\n",
    "MY_EPOCH = 5 # 학습 반복 횟수 (성능 향상 위해 5 이상 권장)\n",
    "MY_BATCH = 128 # 배치 크기 (GPU 메모리에 따라 조절 가능)\n",
    "MY_LEARNING = 0.005 # 학습률 (기본 Adam 기준, 너무 크면 발산 위험)\n",
    "\n",
    "MY_PATCH = 7 # 한 변을 나누는 패치 수 (7x7 → 총 49개 패치)\n",
    "MY_ENCODER = 2 # 인코더 블록 수 (Transformer encoder layer 수)\n",
    "MY_HIDDEN = 8 # 패치 임베딩 차원 수 (작으면 연산량 작아져 표현력 제한됨)\n",
    "MY_HEAD = 2 # 멀티헤드 어텐션에서의 헤드 수 (각 head당 8/2=4차원)\n",
    "MY_MLP = 3 # MLP 확장 비율 (hidden → hidden×3 → hidden)\n",
    "MY_CLASS = 10 # 클래스 수 (MNIST: 숫자 0~9)\n",
    "\n",
    "# GPU 사용 가능하면 GPU로 설정\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"사용 장치:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e72ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 패치로 나누고 16차원 벡터로 변형\n",
    "def patchify(images, n_patches): # batch = 128\n",
    "    n, c, h, w = images.shape\n",
    "    patch_size = h // n_patches # 28 // 7 = 4\n",
    "    \n",
    "    # 각 패치는 4x4=16 픽셀 -> flatten 후 16차원 벡터\n",
    "    # 총 패치 수 : 7x7=49 -> 출력 shape : [128, 49, 16]\n",
    "    patches = torch.zeros(n, n_patches**2, h*w*c//n_patches**2, device=images.device)\n",
    "    \n",
    "    for idx, image in enumerate(images):\n",
    "        for i in range(n_patches): # 세로 방향 7개\n",
    "            for j in range(n_patches): # 가로 방향 7개\n",
    "                patch = image[\n",
    "                    :,\n",
    "                    i*patch_size : (i+1)*patch_size,\n",
    "                    j*patch_size : (j+1)*patch_size,\n",
    "                ] # patch shape : [1,4,4]\n",
    "                patches[idx, i*n_patches+j] = patch.flatten() # 16차원 벡터로 변형, [16]\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fe965c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8921e-01, -2.9326e-01, -1.7874e-01,  3.0636e-01,  3.3474e-01,\n",
       "           4.6896e-01,  7.6117e-02,  3.3755e-01],\n",
       "         [-2.9163e-01, -2.1169e-01, -2.0923e-01,  4.3220e-01,  2.7449e-01,\n",
       "           5.5693e-01, -1.6262e-01,  1.5204e-01],\n",
       "         [-9.0328e-02, -3.7413e-01, -1.7061e-01,  3.4751e-01,  2.6893e-01,\n",
       "           5.2852e-01, -3.4628e-04,  2.7697e-01],\n",
       "         [-2.1937e-01, -3.0520e-01, -1.9674e-01,  3.4087e-01,  1.7090e-01,\n",
       "           6.7290e-01, -1.8733e-01,  1.3153e-01],\n",
       "         [-1.6656e-01, -3.4484e-01, -1.8677e-01,  3.2935e-01,  3.5773e-01,\n",
       "           3.9760e-01, -1.0853e-01,  1.9246e-01],\n",
       "         [-2.7606e-01, -1.9349e-01, -1.9555e-01,  4.0996e-01,  3.0748e-01,\n",
       "           4.4864e-01,  1.1297e-01,  3.6326e-01],\n",
       "         [-3.3424e-01, -1.3318e-01, -1.9835e-01,  3.9397e-01,  2.4094e-01,\n",
       "           5.3476e-01, -1.6465e-01,  1.4677e-01],\n",
       "         [-3.2252e-01, -1.9736e-01, -1.9777e-01,  3.0896e-01,  1.6838e-01,\n",
       "           6.5816e-01, -1.7565e-01,  1.3944e-01],\n",
       "         [-2.8805e-01, -2.2555e-01, -1.9881e-01,  3.3318e-01,  2.3644e-01,\n",
       "           5.3407e-01,  1.0795e-01,  3.5908e-01],\n",
       "         [-2.8438e-01, -2.4926e-01, -1.9736e-01,  2.8929e-01,  1.8897e-01,\n",
       "           5.8325e-01, -1.5999e-01,  1.4924e-01],\n",
       "         [-2.7028e-01, -2.2695e-01, -1.9122e-01,  3.2165e-01,  2.5622e-01,\n",
       "           5.2127e-01, -3.1916e-03,  2.7267e-01],\n",
       "         [-3.4144e-01, -1.9339e-01, -2.0412e-01,  3.0681e-01,  2.9072e-01,\n",
       "           4.6709e-01,  1.3190e-01,  3.7764e-01],\n",
       "         [-3.9315e-01, -1.2847e-01, -2.1002e-01,  3.5014e-01,  2.9412e-01,\n",
       "           5.5023e-01, -1.4528e-01,  1.6772e-01],\n",
       "         [-4.7297e-01, -8.6700e-02, -2.1881e-01,  2.9739e-01,  3.3502e-01,\n",
       "           4.3416e-01,  4.3071e-01,  6.1071e-01],\n",
       "         [-3.9830e-01, -1.4832e-01, -2.1010e-01,  3.0177e-01,  2.3243e-01,\n",
       "           4.9629e-01, -6.3284e-02,  2.2306e-01],\n",
       "         [-4.1415e-01, -1.1895e-01, -2.0355e-01,  2.7115e-01,  1.3019e-01,\n",
       "           6.1822e-01, -6.1913e-02,  2.2324e-01],\n",
       "         [-8.6938e-02, -3.1321e-01, -1.7506e-01,  4.8039e-01,  4.5402e-01,\n",
       "           3.0917e-01,  2.6590e-01,  4.8364e-01],\n",
       "         [-2.3627e-01, -2.6272e-01, -1.9376e-01,  3.5568e-01,  2.2029e-01,\n",
       "           5.9385e-01, -1.6138e-01,  1.5068e-01],\n",
       "         [-3.3767e-01, -1.8342e-01, -1.9643e-01,  2.8570e-01,  1.3405e-01,\n",
       "           7.2706e-01, -3.2856e-01,  2.1001e-02],\n",
       "         [-3.4509e-01, -1.7984e-01, -2.0626e-01,  3.4575e-01,  3.8110e-01,\n",
       "           3.9544e-01,  9.2598e-02,  3.4970e-01],\n",
       "         [-2.8621e-01, -2.4950e-01, -1.9962e-01,  3.1211e-01,  3.3261e-01,\n",
       "           4.2690e-01,  2.0757e-02,  2.9215e-01],\n",
       "         [-2.5283e-01, -2.3478e-01, -1.9657e-01,  3.8954e-01,  4.1690e-01,\n",
       "           4.0955e-01, -2.1421e-02,  2.6518e-01],\n",
       "         [-3.9972e-01, -1.1416e-01, -2.1292e-01,  3.8523e-01,  3.0224e-01,\n",
       "           5.3387e-01, -7.0737e-02,  2.2508e-01],\n",
       "         [-2.7268e-01, -2.1801e-01, -2.0098e-01,  4.0535e-01,  2.7574e-01,\n",
       "           5.7766e-01, -1.7564e-01,  1.4315e-01],\n",
       "         [-5.7693e-01,  2.2684e-02, -2.2827e-01,  3.1381e-01,  3.7960e-01,\n",
       "           4.1801e-01,  9.0862e-02,  3.4959e-01],\n",
       "         [-2.5134e-01, -2.6658e-01, -1.9181e-01,  3.0822e-01,  3.5999e-01,\n",
       "           4.0561e-01,  1.7558e-01,  4.1302e-01],\n",
       "         [-3.3900e-01, -2.0726e-01, -2.0976e-01,  3.4281e-01,  2.8193e-01,\n",
       "           5.0697e-01,  1.0511e-01,  3.5824e-01],\n",
       "         [-3.5871e-01, -2.1802e-01, -2.1182e-01,  2.8019e-01,  3.8057e-01,\n",
       "           4.1179e-01,  5.7906e-02,  3.2331e-01],\n",
       "         [-1.7975e-01, -2.7686e-01, -1.8719e-01,  4.2056e-01,  2.0442e-01,\n",
       "           6.0940e-01, -2.0805e-01,  1.1460e-01],\n",
       "         [-6.1354e-01,  2.9981e-02, -2.3503e-01,  2.9033e-01,  3.5504e-01,\n",
       "           3.9656e-01,  1.7195e-01,  4.0941e-01],\n",
       "         [-2.9004e-01, -2.4062e-01, -1.9853e-01,  3.0114e-01,  3.7415e-01,\n",
       "           3.7652e-01,  1.9832e-01,  4.2998e-01],\n",
       "         [-3.9968e-01, -1.5782e-01, -2.0846e-01,  2.7272e-01,  2.0216e-01,\n",
       "           6.6955e-01, -2.3051e-01,  9.9813e-02],\n",
       "         [-1.7160e-01, -3.0788e-01, -1.8411e-01,  3.6724e-01,  3.0376e-01,\n",
       "           5.4752e-01, -1.3941e-01,  1.7222e-01],\n",
       "         [-4.0148e-01, -1.1663e-01, -2.1958e-01,  4.1184e-01,  2.9813e-01,\n",
       "           5.1288e-01, -8.8920e-02,  2.0914e-01],\n",
       "         [-4.8238e-01, -7.6060e-02, -2.1203e-01,  2.3551e-01,  2.3230e-01,\n",
       "           5.3564e-01, -2.4736e-02,  2.5552e-01],\n",
       "         [-4.9101e-01, -5.4056e-02, -2.1894e-01,  3.2066e-01,  3.8208e-01,\n",
       "           3.6005e-01,  1.5564e-01,  3.9726e-01],\n",
       "         [-5.5616e-01,  1.6299e-02, -2.2614e-01,  3.3140e-01,  3.1224e-01,\n",
       "           4.9583e-01,  8.0348e-03,  2.8504e-01],\n",
       "         [-2.5341e-01, -2.4448e-01, -1.9395e-01,  3.5558e-01,  2.6773e-01,\n",
       "           5.4718e-01, -2.7223e-02,  2.5677e-01],\n",
       "         [-1.8809e-01, -2.7831e-01, -1.8146e-01,  3.5405e-01,  3.5157e-01,\n",
       "           4.2132e-01,  2.9907e-01,  5.0933e-01],\n",
       "         [-5.9022e-01, -2.9625e-03, -2.3545e-01,  2.8919e-01,  4.8811e-01,\n",
       "           2.4225e-01,  3.7338e-01,  5.6514e-01],\n",
       "         [-4.4447e-01, -1.0610e-01, -2.1635e-01,  3.1274e-01,  2.5425e-01,\n",
       "           5.4162e-01, -7.0264e-02,  2.2218e-01],\n",
       "         [-3.3227e-01, -1.8926e-01, -2.0442e-01,  3.4606e-01,  3.7278e-01,\n",
       "           2.9929e-01,  3.5209e-01,  5.4459e-01],\n",
       "         [-2.9950e-01, -1.8107e-01, -1.9990e-01,  4.0055e-01,  1.4035e-01,\n",
       "           7.0018e-01, -2.6649e-01,  6.9024e-02],\n",
       "         [-3.8636e-01, -1.3212e-01, -2.0684e-01,  3.4041e-01,  2.7932e-01,\n",
       "           5.7470e-01, -8.1344e-02,  2.1767e-01],\n",
       "         [-2.4344e-01, -2.5429e-01, -1.9417e-01,  3.7459e-01,  2.7340e-01,\n",
       "           5.4415e-01, -1.2484e-01,  1.8074e-01],\n",
       "         [-1.3801e-01, -2.8879e-01, -1.7830e-01,  4.2997e-01,  3.5443e-01,\n",
       "           3.9418e-01,  2.2919e-01,  4.5410e-01],\n",
       "         [-2.8963e-01, -2.4781e-01, -2.0133e-01,  3.1601e-01,  2.6396e-01,\n",
       "           5.3564e-01, -1.5649e-01,  1.5578e-01],\n",
       "         [-1.4248e-01, -3.2296e-01, -1.7807e-01,  3.6156e-01,  2.0945e-01,\n",
       "           6.7022e-01, -1.7481e-01,  1.4387e-01],\n",
       "         [-5.0729e-01, -4.4550e-02, -2.2066e-01,  3.0421e-01,  3.5435e-01,\n",
       "           4.2358e-01,  2.5998e-01,  4.7909e-01]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multi-head attention 클래스 정의\n",
    "# n_hidden : 임베딩 차원 수, 8\n",
    "# n_heads : 머리 수 , 2\n",
    "\n",
    "class MyMSA(nn.Module):\n",
    "    def __init__(self, n_hidden, n_heads):\n",
    "        super(MyMSA, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        # 각 head 2개가 처리할 차원, 4\n",
    "        d_head = int(n_hidden / n_heads) # 8/2=4\n",
    "        self.d_head = d_head\n",
    "        \n",
    "        # Q, K, V 행렬 계산\n",
    "        self.q_mappings = nn.ModuleList(\n",
    "            [nn.Linear(d_head, d_head) for _ in range(n_heads)]\n",
    "        )\n",
    "        self.k_mappings = nn.ModuleList(\n",
    "            [nn.Linear(d_head, d_head) for _ in range(n_heads)]\n",
    "        )\n",
    "        self.v_mappings = nn.ModuleList(\n",
    "            [nn.Linear(d_head, d_head) for _ in range(n_heads)]\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        # 입력 데이터 모양 : [128, 50, 8]\n",
    "        # 출력 데이터 모양 : [128, 50, 8]\n",
    "        \n",
    "        # 128개 attention 결과 저장\n",
    "        result = []\n",
    "        for sequence in images:\n",
    "            # 각 이미지 당 50개 패티의 계산 결과\n",
    "            seq_result = []\n",
    "            \n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "                \n",
    "                # 입력 데이터 모양 : [50, 4]\n",
    "                seq = sequence[:, head*self.d_head : (head+1)*self.d_head]\n",
    "                \n",
    "                # self attention 계산\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "                \n",
    "                # print(f'Q의 크기 {len(q[0])}')\n",
    "                attention = self.softmax(q @ k.T / (self.d_head**0.5))\n",
    "                attention = attention @ v\n",
    "                #  #print('attention 크기', attention.shape)\n",
    "                seq_result.append(attention)\n",
    "                \n",
    "            # hstack으로 두개의 head 결과 통합\n",
    "            # [50, 4] + [50, 4] = [50, 8]\n",
    "            merge = torch.hstack(seq_result)\n",
    "            # print(f'통합 결과 {merge.shape}')\n",
    "            \n",
    "            # 현재 이미지 종료\n",
    "            result.append(merge)\n",
    "                \n",
    "            # print(f'최종 배치 처리 결과 {len(result)}')\n",
    "            # 결과를 텐서로 전환\n",
    "            final = [torch.unsqueeze(r, dim=0) for r in result]\n",
    "            final = torch.cat(final)\n",
    "            # print(f'attention 결과 데이터 모양 {final.shape}')\n",
    "            \n",
    "            return final\n",
    "        \n",
    "        \n",
    "# 테스트용 코드\n",
    "data = torch.randn(MY_BATCH, 1, 28, 28)\n",
    "y = patchify(data, MY_PATCH)\n",
    "temp = MyMSA(MY_HIDDEN, MY_HEAD)\n",
    "temp(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bb15af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT 인코더 구현\n",
    "# n_hidden : 임베딩 차원 수 , 8\n",
    "# n_heads : 머리 수 , 2\n",
    "\n",
    "class MyEncoder(nn.Module):\n",
    "    def __init__(self, n_hidden, n_heads):\n",
    "        super(MyEncoder, self).__init__()\n",
    "        \n",
    "        # 패치 임베딩 차원\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        # 멀티헤드 어텐션 수\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        # 첫번째 layer normalization 층\n",
    "        self.norm1 = nn.LayerNorm(n_hidden)\n",
    "        \n",
    "        # multi-head attention layer\n",
    "        self.msa = MyMSA(n_hidden, n_heads)\n",
    "        \n",
    "        # 두번째 layer normalization layer\n",
    "        self.norm2 = nn.LayerNorm(n_hidden)\n",
    "        # 최종 multi-layer perceptron layer\n",
    "        self.mlp = nn.Sequential(\n",
    "        nn.Linear(n_hidden, MY_MLP * n_hidden),\n",
    "        nn.GELU(), #가우시안 함수가 음수영역에 적용된 형태의 RELU 변형\n",
    "        nn.Linear(MY_MLP * n_hidden, n_hidden)\n",
    "        )\n",
    "        \n",
    "    # ViT 인코더 구현\n",
    "    def forward(self, x):\n",
    "        out = x + self.msa(self.norm1(x))\n",
    "        out = self.norm2(out) + self.mlp(self.norm2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd94860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6번 블록\n",
    "class MyVIT(nn.Module):\n",
    "    def __init__(self, n_patches, n_encoder, n_hidden, n_heads, n_class, image_shape=(1, 28, 28)):\n",
    "        super(MyVIT, self).__init__()\n",
    "        self.n_patches = n_patches # 7 patches\n",
    "        self.n_encoder = n_encoder # 2 enconders\n",
    "        self.n_heads = n_heads # 2 heads\n",
    "        self.n_hidden = n_hidden # 8 dimension per 1 patch\n",
    "        \n",
    "        # 한 패치의 화소 수 = (패치 한 변의 길이)^2\n",
    "        self.input_d = (image_shape[1] // n_patches) ** 2\n",
    "        print(\"패치 화소 수 :\", self.input_d)\n",
    "        \n",
    "        # 입력 차원(input_d) → 임베딩 차원(n_hidden)으로 변환\n",
    "        self.linear_mapper = nn.Linear(self.input_d, n_hidden)\n",
    "\n",
    "        # 학습 가능한 CLS 토큰 추가: [1, 1, n_hidden]\n",
    "        self.class_token = nn.Parameter(torch.rand(1, 1, n_hidden)) #이미지 하나 당 [1, 1, 8] cls토큰\n",
    "        print(\"CLS 토큰 모양 :\", self.class_token.shape)\n",
    "        \n",
    "        # 포지셔널 인코딩: [1, 패치수+1, n_hidden]\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, n_patches**2 + 1, n_hidden))\n",
    "\n",
    "        # 인코더 블록 여러 개 쌓기\n",
    "        self.encoder = nn.Sequential(*[\n",
    "        MyEncoder(n_hidden, n_heads) for _ in range(n_encoder)\n",
    "        ])\n",
    "        \n",
    "        # 최종 분류기\n",
    "        self.mlp_head = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, images):\n",
    "        B = images.shape[0]\n",
    "        \n",
    "        # 이미지를 패치 시퀀스로 변환 (patchify 함수 선행 필요)\n",
    "        x = patchify(images, self.n_patches) # [B, num_patches, patch_dim]\n",
    "        x = self.linear_mapper(x) # [B, num_patches, n_hidden]\n",
    "        \n",
    "        # CLS 토큰 복제 및 붙이기\n",
    "        cls_tokens = self.class_token.expand(B, -1, -1) # [128, 1, 8] 배치128사이즈로 확장\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # [B, num_patches+1, n_hidden]\n",
    "        \n",
    "        # 포지셔널 인코딩 추가,\n",
    "        x = x + self.pos_embedding # [B, 50, n_hidden]\n",
    "        \n",
    "        # 인코더 통과\n",
    "        x = self.encoder(x) # [B, 50, n_hidden]\n",
    "        \n",
    "        # CLS 토큰만 추출 → 최종 분류\n",
    "        cls_out = x[:, 0] # [B, n_hidden]\n",
    "        logits = self.mlp_head(cls_out) # [B, n_class]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44342d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패치 화소 수 : 16\n",
      "CLS 토큰 모양 : torch.Size([1, 1, 8])\n",
      "[Epoch 1] Loss: 1081.3224 | Accuracy: 10.50%\n",
      "[Epoch 2] Loss: 1079.9939 | Accuracy: 10.76%\n",
      "[Epoch 3] Loss: 1079.8567 | Accuracy: 10.86%\n",
      "[Epoch 4] Loss: 1079.7857 | Accuracy: 10.98%\n"
     ]
    }
   ],
   "source": [
    "# 7번 블록: 학습 루프 + 테스트 평가 + 정확도 그래프\n",
    "train_dataset = MNIST(root=\"./\", train=True, download=True, transform=ToTensor())\n",
    "test_dataset = MNIST(root=\"./\", train=False, download=True, transform=ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=MY_BATCH, shuffle=True) # batch_size = 128\n",
    "test_loader = DataLoader(test_dataset, batch_size=MY_BATCH)\n",
    "\n",
    "model = MyVIT(\n",
    "    n_patches=MY_PATCH, # 7\n",
    "    n_encoder=MY_ENCODER, # 2\n",
    "    n_hidden=MY_HIDDEN, # 8\n",
    "    n_heads=MY_HEAD, # 2\n",
    "    n_class=MY_CLASS, # 10\n",
    "    image_shape=MY_SHAPE # (1, 28, 28)\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=MY_LEARNING) # 학습률: 0.005\n",
    "\n",
    "# 학습 기록용 리스트\n",
    "train_accuracies = []\n",
    "model.train()\n",
    "start = time()\n",
    "\n",
    "for epoch in range(MY_EPOCH): # 총 5 에폭\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader): #train data에서 index, images, labels가 셋으로 제공\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE) # GPU로 전송\n",
    "        outputs = model(images) # 출력 shape: [128, 10]\n",
    "        loss = criterion(outputs, labels)\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        epoch_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    accuracy = 100 * correct / total\n",
    "    train_accuracies.append(accuracy)\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {epoch_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"총 학습 시간:\", round(time() - start, 2), \"초\")\n",
    "\n",
    "# 정확도 그래프 출력\n",
    "plt.plot(range(1, MY_EPOCH + 1), train_accuracies, marker='o')\n",
    "plt.title(\"Training Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 테스트 평가\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "print(f\"[Test Accuracy] {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
